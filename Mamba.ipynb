{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 29 19:02:12 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 41%   45C    P8    15W / 140W |   1107MiB / 16376MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mamba-SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.9/dist-packages (1.1.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (4.37.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (23.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (0.7.0)\n",
      "Requirement already satisfied: causal-conv1d>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (1.1.1)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (1.11.1.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (1.12.1+cu116)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.9/dist-packages (from mamba-ssm) (2.2.0)\n",
      "Requirement already satisfied: buildtools in /usr/local/lib/python3.9/dist-packages (from causal-conv1d>=1.1.0->mamba-ssm) (1.0.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->mamba-ssm) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (1.23.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (3.9.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (0.15.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.9/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers->mamba-ssm) (2023.12.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (2.8.2)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: redo in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (2.0.4)\n",
      "Requirement already satisfied: furl in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (2.1.3)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (1.4.41)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (3.19.2)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (3.1.2)\n",
      "Requirement already satisfied: twisted in /usr/local/lib/python3.9/dist-packages (from buildtools->causal-conv1d>=1.1.0->mamba-ssm) (23.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->mamba-ssm) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->mamba-ssm) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->mamba-ssm) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->mamba-ssm) (2.1.1)\n",
      "Requirement already satisfied: six>=1.8.0 in /usr/lib/python3/dist-packages (from furl->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (1.14.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from furl->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (2.1.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (2.0.1)\n",
      "Requirement already satisfied: zope-interface>=5 in /usr/local/lib/python3.9/dist-packages (from twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (6.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.9/dist-packages (from twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (23.10.4)\n",
      "Requirement already satisfied: automat>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (22.10.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /usr/local/lib/python3.9/dist-packages (from twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.9/dist-packages (from twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (21.0.0)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.9/dist-packages (from twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (23.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from zope-interface>=5->twisted->buildtools->causal-conv1d>=1.1.0->mamba-ssm) (66.1.1)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install causal-conv1d>=1.1.0\n",
    "!pip install transformers==4.37.2\n",
    "!pip install beautifulsoup4\n",
    "!pip install mamba-ssm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "\n",
    "# Load model\n",
    "model = MambaLMHeadModel.from_pretrained(\n",
    "  \"state-spaces/mamba-1.4b\", \n",
    "  device=\"cuda\", \n",
    "  dtype=torch.bfloat16)\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A conversation between a user and a smart AI assistant.\n",
      "\n",
      "### User: Hello!\n",
      "### Assistant: Hello!\n",
      "\n",
      "### User: I'm hungry.\n",
      "### Assistant: I'm hungry.\n",
      "\n",
      "### User: I'm thirsty.\n",
      "### Assistant: I'm thirsty.\n",
      "\n",
      "### User: I'm tired.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\\\n",
    "\"\"\"A conversation between a user and a smart AI assistant.\n",
    "\n",
    "### User: Hello!\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# from https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py#L54\n",
    "output_tokenized = model.generate(\n",
    "    input_ids=prompt_tokenized[\"input_ids\"], \n",
    "    max_length=70,\n",
    "    cg=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    "    top_p=0.1,\n",
    "    )\n",
    "output=tokenizer.decode(output_tokenized[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decode() got an unexpected keyword argument 'return_hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m prompt_tokenized\u001b[38;5;241m=\u001b[39mtokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# from https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py#L54\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m output_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_timing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m output\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output_tokenized[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/mamba_ssm/utils/generation.py:244\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, top_k, top_p, temperature, return_dict_in_generate, output_scores, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    243\u001b[0m ):\n\u001b[0;32m--> 244\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_scores:\n\u001b[1;32m    248\u001b[0m         output\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: decode() got an unexpected keyword argument 'return_hidden_states'"
     ]
    }
   ],
   "source": [
    "prompt=\\\n",
    "\"\"\"Tell me about the history of India.\"\"\"\n",
    "\n",
    "# \"\"\"Which are the best places to visit in India?\"\"\"\n",
    "\n",
    "prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# from https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py#L54\n",
    "output_tokenized = model.generate(\n",
    "    input_ids=prompt_tokenized[\"input_ids\"], \n",
    "    max_length=50,\n",
    "    cg=True,\n",
    "    output_scores=True,\n",
    "    enable_timing=False,\n",
    "    temperature=0.5,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    return_dict_in_generate=True,\n",
    "    return_hidden_states=True,\n",
    "    )\n",
    "output=tokenizer.decode(output_tokenized[0][0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateDecoderOnlyOutput(sequences=tensor([[17570,   479,   670,   253,  2892,   273,  5427,    15,   187,   187,\n",
       "           510,  2892,   273,  5427,   310,   247,  1077,  9542,  2892,    15,\n",
       "           733,   556,   644,   247,  1077,  9542,  2892,    15,   380,  2892,\n",
       "           273,  5427,   310,  1077,  2570,    15,   733,   556,   644,   247,\n",
       "          1077,  2570,  2892,    15,   733,   556,   644,   247,  1077,  2570,\n",
       "          2892,    15,   733,   556,   644,   247,  1077,  2570,  2892,    15,\n",
       "           733,   556,   644,   247,  1077,  2570,  2892,    15,   733,   556]],\n",
       "       device='cuda:0'), scores=(tensor([[  2.7969, -16.2500,  -2.5781,  ..., -15.3750, -15.6250, -15.5000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  7.3438, -10.1250,   1.4297,  ...,  -9.7500,  -9.9375,  -9.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ 3.0312, -0.0217, 10.1875,  ...,  0.5195,  0.4160,  0.7812]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  6.1875, -10.5625,   6.6562,  ...,  -9.4375,  -9.8750,  -9.7500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  3.0938, -11.6875,   8.0000,  ..., -11.3125, -11.3125, -11.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -2.0156, -21.8750,  -4.0625,  ..., -21.0000, -21.1250, -21.0000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ 7.0938, -9.4375, 13.0625,  ..., -8.6875, -8.6875, -8.7500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[15.0625, -3.1406, 15.3750,  ..., -2.7188, -2.8906, -2.6406]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[18.5000,  1.2891, 16.6250,  ...,  1.9844,  1.5781,  2.2344]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[23.3750,  5.3438, 22.8750,  ...,  5.3750,  4.9062,  5.6562]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -3.4062, -25.3750,  -2.3750,  ..., -24.7500, -25.2500, -24.7500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ 3.4219, -7.1562,  8.8750,  ..., -6.7812, -6.7812, -7.0312]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  4.2812, -13.2500,  -1.6406,  ..., -12.8750, -12.6250, -13.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[10.0000, -6.6562,  9.8750,  ..., -6.2188, -6.0312, -6.5312]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.9688, -15.0625,   4.8125,  ..., -14.7500, -14.4375, -14.9375]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[25.0000,  8.3750, 25.8750,  ...,  8.1250,  8.5000,  8.6875]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[23.1250,  3.8906, 19.7500,  ...,  4.4375,  4.1250,  4.3125]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[28.2500,  8.7500, 25.3750,  ...,  8.5625,  8.3125,  8.6250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  8.0625, -15.5000,   6.9375,  ..., -14.8125, -14.8750, -14.7500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  0.0757, -11.1875,   4.6250,  ..., -11.1875, -10.7500, -11.3125]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.1250, -12.7500,  -1.7031,  ..., -12.6875, -12.2500, -13.0625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[14.0000, -4.2812, 11.9375,  ..., -3.3281, -3.7812, -3.7969]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  2.8281, -13.5625,   3.5156,  ..., -13.2500, -13.1875, -13.5000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  4.2812, -17.0000,   0.2324,  ..., -16.2500, -16.3750, -16.5000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.8750, -10.9375,   8.3125,  ..., -10.3750, -10.1875, -10.6250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  6.0625, -14.0625,   5.5938,  ..., -13.7500, -13.6875, -13.6875]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[24.6250,  7.3125, 24.6250,  ...,  7.2812,  7.0000,  7.6875]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  1.9609, -13.3750,   5.2188,  ..., -12.8125, -13.0000, -13.0625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  4.7188, -13.8125,  -2.7969,  ..., -13.6875, -13.3125, -14.0625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.9688, -10.6875,   5.7188,  ..., -10.4375, -10.1250, -10.6875]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.0312, -16.1250,   4.2188,  ..., -15.8750, -15.4375, -16.0000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[20.3750,  2.2812, 22.3750,  ...,  2.1094,  2.6094,  2.5625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[19.0000, -0.6172, 17.0000,  ...,  0.0957, -0.0908, -0.2080]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[24.0000,  3.4688, 22.1250,  ...,  3.5312,  3.2188,  3.4062]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  4.1250, -19.1250,   4.2188,  ..., -18.3750, -18.5000, -18.3750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ 2.6406, -8.4375,  6.1875,  ..., -8.4375, -8.0625, -8.5625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  3.9688, -14.8125,  -3.9844,  ..., -14.7500, -14.3750, -15.1875]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  1.0781, -15.3750,   0.4902,  ..., -15.3750, -14.8750, -15.6250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  0.6133, -20.3750,  -0.3887,  ..., -20.1250, -19.5000, -20.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  2.9688, -17.3750,   4.2812,  ..., -17.3750, -16.8750, -17.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[13.9375, -7.9688, 11.7500,  ..., -7.1250, -7.2500, -7.6250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[15.0625, -8.1875, 13.2500,  ..., -7.9688, -8.3125, -8.3125]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  3.8906, -20.5000,   2.9219,  ..., -19.7500, -19.8750, -19.8750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ 4.4375, -7.5938,  7.5938,  ..., -7.3750, -7.1875, -7.5938]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  2.5781, -17.6250,  -6.5312,  ..., -17.5000, -17.1250, -17.8750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  0.8320, -15.4375,  -0.3008,  ..., -15.5625, -15.1250, -15.8750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -1.1328, -22.7500,  -3.0938,  ..., -22.6250, -22.0000, -22.8750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -4.3750, -24.2500,  -4.7500,  ..., -24.2500, -23.7500, -24.0000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  3.2344, -21.1250,   0.6680,  ..., -20.5000, -20.3750, -21.0000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  7.0938, -18.5000,   4.9062,  ..., -18.1250, -18.3750, -18.5000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  7.7812, -18.0000,   5.4062,  ..., -17.2500, -17.3750, -17.3750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[12.8125, -1.3359, 14.0625,  ..., -0.8359, -0.7578, -1.0547]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  2.9688, -17.7500,  -6.9375,  ..., -17.7500, -17.2500, -18.1250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.2188, -13.0000,   2.7656,  ..., -13.2500, -12.6875, -13.4375]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  8.6250, -14.4375,   5.9062,  ..., -14.3750, -13.8125, -14.6250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  3.3594, -18.2500,   1.6641,  ..., -18.1250, -17.8750, -18.0000]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  6.6250, -19.3750,   3.5000,  ..., -18.6250, -18.6250, -19.1250]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  8.5000, -17.8750,   5.7500,  ..., -17.5000, -17.7500, -17.7500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[ 14.0000, -12.1875,  10.3125,  ..., -11.3125, -11.3750, -11.5625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[14.6250, -1.5938, 14.6875,  ..., -0.9570, -0.9102, -1.2109]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  5.0625, -15.5625,  -4.8750,  ..., -15.6250, -15.1250, -15.8750]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[  8.5625, -11.9375,   4.5938,  ..., -12.1250, -11.6250, -12.3125]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)), attentions=None, hidden_states=None, past_key_values=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_tokenized[1]) # number of tokens in output\n",
    "# output_tokenized[1][0].shape # scores for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2048])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = model.backbone(prompt_tokenized[\"input_ids\"], \n",
    "            #    inference_params=inference_params\n",
    "               )\n",
    "\n",
    "hidden_state.shape # batch, seq_len, dim (B, L, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Mamba hidden states used for classification\n",
    "- Fine-tune mamba for classification or use mamba as feature extractor\n",
    "- Mamba fine-tuned for QA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
